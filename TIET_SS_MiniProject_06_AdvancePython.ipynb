{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2kEGsDfYuxbmZC94ghxSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManthanVerma7/TIET-SS-MiniProject-06-AdvancePython/blob/main/TIET_SS_MiniProject_06_AdvancePython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Process Automation**"
      ],
      "metadata": {
        "id": "CHOBFHxXIQBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ylD9AjGIDOJ"
      },
      "outputs": [],
      "source": [
        "# Q1: Create a file that contains 1000 lines of random strings.\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "with open(\"q1_random_1000_lines.txt\", \"w\") as f:\n",
        "    for _ in range(1000):\n",
        "        random_str = ''.join(random.choices(string.ascii_letters + string.digits, k=20))\n",
        "        f.write(random_str + '\\n')\n",
        "\n",
        "print(\"Created 'q1_random_1000_lines.txt'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: Create a file that contains multiple lines of random strings and file size must be 5 MB.\n",
        "\n",
        "with open(\"q2_random_5mb.txt\", \"w\") as f:\n",
        "    while f.tell() < 5 * 1024 * 1024:  # 5 MB\n",
        "        line = ''.join(random.choices(string.ascii_letters + string.digits, k=100))\n",
        "        f.write(line + '\\n')\n"
      ],
      "metadata": {
        "id": "x-zueFFNJCdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: Create 10 files that contain multiple lines of random strings and each file must be 5 MB.\n",
        "\n",
        "for i in range(10):\n",
        "    with open(f\"q3_file_{i+1}.txt\", \"w\") as f:\n",
        "        while f.tell() < 5 * 1024 * 1024:\n",
        "            line = ''.join(random.choices(string.ascii_letters + string.digits, k=100))\n",
        "            f.write(line + '\\n')\n"
      ],
      "metadata": {
        "id": "HmUbLGqjJbxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: Create 5 files of size 1GB to 5GB containing multiple lines of random strings.\n",
        "\n",
        "sizes_gb = [1, 2, 3, 4, 5]\n",
        "\n",
        "for size in sizes_gb:\n",
        "    with open(f\"q4_{size}GB.txt\", \"w\") as f:\n",
        "        target_bytes = size * 1024 * 1024 * 1024\n",
        "        while f.tell() < target_bytes:\n",
        "            line = ''.join(random.choices(string.ascii_letters + string.digits, k=100))\n",
        "            f.write(line + '\\n')\n"
      ],
      "metadata": {
        "id": "RMzs46XSJd7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: Convert all the files of Q4 into upper case one by one.\n",
        "\n",
        "import os\n",
        "\n",
        "for size in sizes_gb:\n",
        "    filename = f\"q4_{size}GB.txt\"\n",
        "    with open(filename, \"r\") as f:\n",
        "        content = f.read().upper()\n",
        "    with open(f\"q5_upper_{size}GB.txt\", \"w\") as f:\n",
        "        f.write(content)\n"
      ],
      "metadata": {
        "id": "068MNyelJgWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6: Convert all the files of Q4 into upper case using multi-threading.\n",
        "\n",
        "import threading\n",
        "\n",
        "def convert_upper_thread(file_size):\n",
        "    input_file = f\"q4_{file_size}GB.txt\"\n",
        "    output_file = f\"q6_thread_upper_{file_size}GB.txt\"\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read().upper()\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(data)\n",
        "\n",
        "threads = []\n",
        "for size in sizes_gb:\n",
        "    t = threading.Thread(target=convert_upper_thread, args=(size,))\n",
        "    t.start()\n",
        "    threads.append(t)\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n"
      ],
      "metadata": {
        "id": "8x6xNW1DJkrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: Automatically download 10 images of cat from Google Images.\n",
        "\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "\n",
        "google_crawler = GoogleImageCrawler(storage={'root_dir': 'q7_cat_images'})\n",
        "google_crawler.crawl(keyword='cat', max_num=10)\n",
        "\n",
        "print(\"✅ Q7 complete: 10 cat images downloaded to 'q7_cat_images/'\")\n"
      ],
      "metadata": {
        "id": "R8EVQDaRJjM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8: Automatically download 10 videos of \"Machine Learning\" from YouTube.\n",
        "\n",
        "from pytube import Search, YouTube\n",
        "import os\n",
        "\n",
        "search = Search(\"Machine Learning\")\n",
        "results = search.results[:10]\n",
        "\n",
        "os.makedirs(\"q8_ml_videos\", exist_ok=True)\n",
        "\n",
        "for i, video in enumerate(results):\n",
        "    stream = video.streams.filter(progressive=True, file_extension='mp4').first()\n",
        "    stream.download(output_path=\"q8_ml_videos\", filename=f\"ml_video_{i+1}.mp4\")\n",
        "\n",
        "print(\"✅ Q8 complete: 10 Machine Learning videos downloaded.\")\n"
      ],
      "metadata": {
        "id": "AvD4n5h4J0zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9: Convert all the videos from Q8 to audio (MP3 format).\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "\n",
        "video_folder = \"q8_ml_videos\"\n",
        "audio_folder = \"q9_audio_output\"\n",
        "os.makedirs(audio_folder, exist_ok=True)\n",
        "\n",
        "for video_file in os.listdir(video_folder):\n",
        "    if video_file.endswith(\".mp4\"):\n",
        "        video_path = os.path.join(video_folder, video_file)\n",
        "        audio_path = os.path.join(audio_folder, video_file.replace(\".mp4\", \".mp3\"))\n",
        "\n",
        "        video_clip = VideoFileClip(video_path)\n",
        "        video_clip.audio.write_audiofile(audio_path)\n",
        "\n",
        "print(\"✅ Q9 complete: Audio files saved in 'q9_audio_output/'\")\n"
      ],
      "metadata": {
        "id": "hFB4yle9J4AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10: Automated pipeline using multi-threading to download 100 YouTube videos and convert to audio.\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pytube import Search\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "search_results = Search(\"Machine Learning\").results[:100]\n",
        "os.makedirs(\"q10_pipeline_videos\", exist_ok=True)\n",
        "os.makedirs(\"q10_pipeline_audio\", exist_ok=True)\n",
        "\n",
        "def download_and_convert(video_obj, index):\n",
        "    try:\n",
        "        video = video_obj.streams.filter(progressive=True, file_extension='mp4').first()\n",
        "        video_path = video.download(output_path=\"q10_pipeline_videos\", filename=f\"video_{index}.mp4\")\n",
        "\n",
        "        clip = VideoFileClip(video_path)\n",
        "        clip.audio.write_audiofile(f\"q10_pipeline_audio/audio_{index}.mp3\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {index}: {e}\")\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    for i, video_obj in enumerate(search_results):\n",
        "        executor.submit(download_and_convert, video_obj, i)\n",
        "\n",
        "print(\"✅ Q10 complete: Downloaded and converted 100 videos to audio.\")\n"
      ],
      "metadata": {
        "id": "TZknufDMJ3-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11: Download 500 Dog images → Rescale them to 50%.\n",
        "\n",
        "from PIL import Image\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "\n",
        "download_dir = 'q11_dog_images'\n",
        "resized_dir = 'q11_resized_dogs'\n",
        "\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "os.makedirs(resized_dir, exist_ok=True)\n",
        "\n",
        "# Step 1: Download\n",
        "google_crawler = GoogleImageCrawler(storage={'root_dir': download_dir})\n",
        "google_crawler.crawl(keyword='dog', max_num=500)\n",
        "\n",
        "# Step 2: Rescale\n",
        "for img_file in os.listdir(download_dir):\n",
        "    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        img_path = os.path.join(download_dir, img_file)\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            new_size = (img.size[0] // 2, img.size[1] // 2)\n",
        "            resized = img.resize(new_size)\n",
        "            resized.save(os.path.join(resized_dir, img_file))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(\"✅ Q11 complete: 500 Dog images downloaded and resized to 50%.\")\n"
      ],
      "metadata": {
        "id": "VaQFhZEDJ-Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Data Analytics**"
      ],
      "metadata": {
        "id": "Bt-V6cz1KHcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q12: Create a random dataset of 100 rows and 30 columns with values between [1,200]\n",
        "# (i) Replace values between 10 and 60 with NA and count rows with missing values\n",
        "# (ii) Replace NA with column average\n",
        "# (iii) Pearson correlation + heatmap, list columns with correlation < -0.7\n",
        "# (iv) Normalize all values between 0 and 10\n",
        "# (v) Replace values <= 0.5 with 1, else with 0\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset generation\n",
        "df = pd.DataFrame(np.random.randint(1, 201, size=(100, 30)))\n",
        "\n",
        "# (i) Replace values in [10,60] with NaN\n",
        "df_replaced = df.mask(df.between(10, 60))\n",
        "missing_rows = df_replaced.isnull().any(axis=1).sum()\n",
        "print(f\"(i) Rows with missing values: {missing_rows}\")\n",
        "\n",
        "# (ii) Replace NaN with column mean\n",
        "df_filled = df_replaced.fillna(df_replaced.mean())\n",
        "\n",
        "# (iii) Pearson correlation matrix & heatmap\n",
        "correlation_matrix = df_filled.corr(method='pearson')\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
        "plt.title(\"Pearson Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Columns with correlation < -0.7 with any other column\n",
        "low_corr = (correlation_matrix < -0.7).sum()\n",
        "low_corr_columns = low_corr[low_corr > 0].index.tolist()\n",
        "print(f\"(iii) Columns with correlation < -0.7: {low_corr_columns}\")\n",
        "\n",
        "# (iv) Normalize between 0 and 10\n",
        "df_normalized = 10 * (df_filled - df_filled.min()) / (df_filled.max() - df_filled.min())\n",
        "\n",
        "# (v) Replace with 1 if value <= 0.5 else 0\n",
        "df_binary = df_normalized.applymap(lambda x: 1 if x <= 0.5 else 0)\n"
      ],
      "metadata": {
        "id": "3AiAc2ILKM9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13: Create 500×10 dataset\n",
        "# Columns 1-4: [-10, 10], Columns 5-8: [10, 20], Columns 9-10: [-100, 100]\n",
        "# Apply: K-Means and Hierarchical clustering with graphs\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dataset creation\n",
        "data = pd.concat([\n",
        "    pd.DataFrame(np.random.uniform(-10, 10, size=(500, 4))),\n",
        "    pd.DataFrame(np.random.uniform(10, 20, size=(500, 4))),\n",
        "    pd.DataFrame(np.random.uniform(-100, 100, size=(500, 2)))\n",
        "], axis=1)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# (i) K-Means clustering - Elbow method\n",
        "inertia = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.title(\"Elbow Method for K-Means\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# (ii) Hierarchical clustering - Dendrogram\n",
        "linked = linkage(data_scaled, method='ward')\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linked, truncate_mode='lastp', p=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0tor179vKM7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13: Create 500×10 dataset\n",
        "# Columns 1-4: [-10, 10], Columns 5-8: [10, 20], Columns 9-10: [-100, 100]\n",
        "# Apply: K-Means and Hierarchical clustering with graphs\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dataset creation\n",
        "data = pd.concat([\n",
        "    pd.DataFrame(np.random.uniform(-10, 10, size=(500, 4))),\n",
        "    pd.DataFrame(np.random.uniform(10, 20, size=(500, 4))),\n",
        "    pd.DataFrame(np.random.uniform(-100, 100, size=(500, 2)))\n",
        "], axis=1)\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# (i) K-Means clustering - Elbow method\n",
        "inertia = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.title(\"Elbow Method for K-Means\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# (ii) Hierarchical clustering - Dendrogram\n",
        "linked = linkage(data_scaled, method='ward')\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(linked, truncate_mode='lastp', p=10)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QhufYxDIKM2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14: Create dataset of size 600×15 with values in [-100, 100]\n",
        "# Plot: (i) Scatter of Col 5 vs Col 6, (ii) Histogram of all, (iii) Box plot of all\n",
        "\n",
        "df = pd.DataFrame(np.random.uniform(-100, 100, size=(600, 15)))\n",
        "\n",
        "# (i) Scatter plot of Column 5 vs Column 6\n",
        "plt.scatter(df[4], df[5], alpha=0.6)\n",
        "plt.xlabel(\"Column 5\")\n",
        "plt.ylabel(\"Column 6\")\n",
        "plt.title(\"Scatter Plot: Column 5 vs Column 6\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# (ii) Histograms\n",
        "df.hist(figsize=(14, 10), bins=20, edgecolor='black')\n",
        "plt.suptitle(\"Histogram of All Columns\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# (iii) Box Plot\n",
        "df.plot(kind='box', figsize=(14, 6), vert=False)\n",
        "plt.title(\"Box Plot of All Columns\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "skAM5GmbKM1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15: Create dataset (500×5) with values [5,10]\n",
        "# (i) T-test, (ii) Wilcoxon Signed Rank, (iii) Two Sample T-Test & Wilcoxon Rank-Sum\n",
        "\n",
        "from scipy.stats import ttest_1samp, wilcoxon, ttest_ind, ranksums\n",
        "\n",
        "df = pd.DataFrame(np.random.uniform(5, 10, size=(500, 5)))\n",
        "\n",
        "# (i) T-Test against mean = 7.5\n",
        "for col in df.columns:\n",
        "    t_stat, p_val = ttest_1samp(df[col], 7.5)\n",
        "    print(f\"Column {col}: T-Test p-value = {p_val:.4f}\")\n",
        "\n",
        "# (ii) Wilcoxon Signed-Rank Test vs 7.5\n",
        "for col in df.columns:\n",
        "    try:\n",
        "        w_stat, p_val = wilcoxon(df[col] - 7.5)\n",
        "        print(f\"Column {col}: Wilcoxon p-value = {p_val:.4f}\")\n",
        "    except:\n",
        "        print(f\"Column {col}: Wilcoxon test not applicable (tie values)\")\n",
        "\n",
        "# (iii) Two-Sample T-Test & Wilcoxon between Column 3 and 4\n",
        "print(\"\\nColumn 3 vs Column 4:\")\n",
        "print(\"T-Test:\", ttest_ind(df[2], df[3]))\n",
        "print(\"Wilcoxon Rank-Sum:\", ranksums(df[2], df[3]))\n"
      ],
      "metadata": {
        "id": "Z9MUSj8TKMta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}